{
  "name": "loquacious-server",
  "version": "1.0.0",
  "scripts": {
    "start": "ts-node -r tsconfig-paths/register src/server.ts",
    "dev": "nodemon -r tsconfig-paths/register --watch src src/server.ts"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "Server component that can connect to OpenAI or local llama.cpp server",
  "dependencies": {
    "cors": "^2.8.5",
    "dotenv": "^16.4.5",
    "elevenlabs": "^0.16.1",
    "express": "^4.21.0",
    "fluent-ffmpeg": "^2.1.3",
    "openai": "^4.67.0"
  },
  "devDependencies": {
    "@swc/core": "^1.7.35",
    "@swc/helpers": "^0.5.13",
    "@types/cors": "^2.8.17",
    "@types/dotenv": "^8.2.0",
    "@types/express": "^4.17.21",
    "@types/fluent-ffmpeg": "^2.1.26",
    "@types/node": "^22.7.5",
    "regenerator-runtime": "^0.14.1",
    "ts-node": "^10.9.2",
    "tsconfig-paths": "^4.2.0",
    "typescript": "^5.6.3"
  }
}
